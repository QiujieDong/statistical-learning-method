{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章 提升方法\n",
    "本章将实现Adaboost, 并复现例8.1。最后调用sklearn.ensemble.AdaBoostClassifier库函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boost\n",
    "\n",
    "“装袋”（bagging）和“提升”（boost）是构建组合模型的两种最主要的方法，所谓的组合模型是由多个基本模型构成的模型，组合模型的预测效果往往比任意一个基本模型的效果都要好。\n",
    "\n",
    "- 装袋：每个基本模型由从总体样本中随机抽样得到的不同数据集进行训练得到，通过重抽样得到不同训练数据集的过程称为装袋。\n",
    "\n",
    "- 提升：每个基本模型训练时的数据集采用不同权重，针对上一个基本模型分类错误的样本增加权重，使得新的模型重点关注误分类样本\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "AdaBoost是AdaptiveBoost的缩写，表明该算法是具有适应性的提升算法。\n",
    "\n",
    "算法的步骤如下：\n",
    "\n",
    "1）给每个训练样本（$x_{1},x_{2},….,x_{N}$）分配权重，初始权重$w_{1}$均为1/N。\n",
    "\n",
    "2）针对带有权值的样本进行训练，得到模型$G_m$（初始模型为G1）。\n",
    "\n",
    "3）计算模型$G_m$的误分率$e_m=\\sum_{i=1}^Nw_iI(y_i\\not= G_m(x_i))=\\sum_{y_i\\not= G_m(x_i)}w_i$\n",
    "\n",
    "4）计算模型$G_m$的系数$\\alpha_m=\\frac{1}{2}\\log(\\frac{1-e_m}{e_m})$\n",
    "\n",
    "5）根据误分率e和当前权重向量$w_m$更新权重向量$w_{m+1}$。\n",
    "\n",
    "6）计算组合模型$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x_i)$的误分率。\n",
    "\n",
    "7）当组合模型的误分率或迭代次数低于一定阈值，停止迭代；否则，回到步骤2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "def create_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "    data = np.array(df.iloc[:100, [0, 1, -1]])\n",
    "    data[:, -1] = [1 if d == 1 else -1 for d in data[:, -1]]\n",
    "    return data[:, :2], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5224f2a828>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:50, 0], X[:50,1], label='-1')\n",
    "plt.scatter(X[50:, 0], X[50:,1], label='1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50, learning_rate=1.0):\n",
    "        self.clf_num = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def init_args(self, datasets, labels):\n",
    "        self.X = datasets\n",
    "        self.Y = labels\n",
    "        self.M, self.N = datasets.shape\n",
    "        \n",
    "        # weak classifier number and set\n",
    "        self.clf_sets = []\n",
    "        \n",
    "        # initialization the weights\n",
    "        self.weights = [1.0 / self.M] * self.M\n",
    "        \n",
    "        # G(x)’s coefficient - alpha\n",
    "        self.alpha = []\n",
    "        \n",
    "    def _G(self, features, labels, weights):\n",
    "        m = len(features)\n",
    "        error = 100000.0\n",
    "        best_v = 0.0\n",
    "        \n",
    "        # single dimension features\n",
    "        feature_min = min(features)\n",
    "        feature_max = max(features)\n",
    "        n_step = (feature_max - feature_min + self.learning_rate) // self.learning_rate\n",
    "        direct, compare_array = None, None\n",
    "        for i in range(1, int(n_step)):\n",
    "            v = feature_min + self.learning_rate * i\n",
    "            \n",
    "            if v not in features:\n",
    "                # misclassification calculation\n",
    "                compare_array_positive = np.array([1 if features[k] > v else -1 for k in range(m)])\n",
    "                weight_error_positive = sum([weights[k] for k in range(m) if compare_array_positive[k] != labels[k]])\n",
    "                \n",
    "                compare_array_negative = np.array([-1 if features[k] > v else 1 for k in range(m)])\n",
    "                weight_error_negative = sum([weights[k] for k in range(m) if compare_array_negative[k] != labels[k]])\n",
    "                \n",
    "                if weight_error_positive < weight_error_negative:\n",
    "                    weight_error = weight_error_positive\n",
    "                    _compare_array = compare_array_positive\n",
    "                    direct = 'positive'\n",
    "                else:\n",
    "                    weight_error = weight_error_negative\n",
    "                    _compare_array = compare_array_negative\n",
    "                    direct = 'negative'\n",
    "                    \n",
    "                if weight_error < error:\n",
    "                    error = weight_error\n",
    "                    compare_array = _compare_array\n",
    "                    best_v = v\n",
    "        return best_v, direct, error, compare_array\n",
    "    \n",
    "    #calculating alpha\n",
    "    def _alpha(self, error):\n",
    "        return 0.5 * np.log((1 - error) / error)\n",
    "    \n",
    "    # regularization factor\n",
    "    def _Z(self, weights, a, clf):\n",
    "        return sum([weights[i] * np.exp(-1 * a * self.Y[i] * clf[i]) for i in range(self.M)])\n",
    "\n",
    "    # updating weights\n",
    "    def _w(self, a, clf, Z):\n",
    "        for i in range(self.M):\n",
    "            self.weights[i] = self.weights[i] * np.exp(-1 * a * self.Y[i] * clf[i]) / Z\n",
    "            \n",
    "    # f(x) is a linear combination of G(x)\n",
    "    def _f(self, alpha, clf_sets):\n",
    "        pass\n",
    "    \n",
    "    def G(self, x, v, direct):\n",
    "        if direct == 'positive':\n",
    "            return 1 if x > v else -1\n",
    "        else:\n",
    "            return -1 if x > v else 1\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.init_args(X, y)\n",
    "        \n",
    "        for epoch in range(self.clf_num):\n",
    "            best_clf_error, best_v, clf_result = 100000.0, None, None\n",
    "            \n",
    "            # selecting the minimum error based on  the features' dimension\n",
    "            for j in range(self.N):\n",
    "                features = self.X[:, j]\n",
    "                # classification threshold, classification error, classification result\n",
    "                v, direct, error, compare_array = self._G(features, self.Y, self.weights)\n",
    "                \n",
    "                if error < best_clf_error:\n",
    "                    best_clf_error = error\n",
    "                    best_v = v\n",
    "                    final_direct = direct\n",
    "                    clf_result = compare_array\n",
    "                    axis = j\n",
    "                    \n",
    "                if best_clf_error == 0:\n",
    "                    break\n",
    "                    \n",
    "            # calculating the G(x)'s coefficient - a\n",
    "            a = self._alpha(best_clf_error)\n",
    "            self.alpha.append(a)\n",
    "            # record the classifier\n",
    "            self.clf_sets.append((axis, best_v, final_direct))\n",
    "            #regularization factor\n",
    "            Z = self._Z(self.weights, a, clf_result)\n",
    "            #updating weights\n",
    "            self._w(a, clf_result, Z)\n",
    "            \n",
    "    def predict(self, feature):\n",
    "        result = 0.0\n",
    "        for i in range(len(self.clf_sets)):\n",
    "            axis, clf_v, direct = self.clf_sets[i]\n",
    "            f_input = feature[axis]\n",
    "            result += self.alpha[i] * self.G(f_input, clf_v, direct)\n",
    "            \n",
    "        return 1 if result > 0 else -1\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        right_count = 0\n",
    "        for i in range(len(X_test)):\n",
    "            feature = X_test[i]\n",
    "            if self.predict(feature) == y_test[i]:\n",
    "                right_count += 1\n",
    "        return right_count / len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(10).reshape(10, 1)\n",
    "y = np.array([1, 1, 1, -1, -1, -1, 1, 1, 1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoost(n_estimators=3, learning_rate=0.5)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoost(n_estimators=10, learning_rate=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score:64.800%\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(1, 101):\n",
    "    X, y = create_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    clf = AdaBoost(n_estimators=100, learning_rate=0.2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    r = clf.score(X_test, y_test)\n",
    "    result.append(r)\n",
    "    \n",
    "print('Average score:{:.3f}%'.format(sum(result) / len(result) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# sklearn.ensemble.AdaBoostClassifier\n",
    "\n",
    "- algorithm：这个参数只有AdaBoostClassifier有。主要原因是scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用了和我们的原理篇里二元分类Adaboost算法的扩展，即用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。我们一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。\n",
    "\n",
    "- n_estimators： AdaBoostClassifier和AdaBoostRegressor都有，就是我们的弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。\n",
    "\n",
    "-  learning_rate:  AdaBoostClassifier和AdaBoostRegressor都有，即每个弱学习器的权重缩减系数ν\n",
    "\n",
    "- base_estimator：AdaBoostClassifier和AdaBoostRegressor都有，即我们的弱分类学习器或者弱回归学习器。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.5,\n",
       "                   n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
